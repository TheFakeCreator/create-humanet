# Evaluations

This directory contains AI validation reports and evaluation results.

## Purpose

Evaluations are generated automatically by the Humanet platform's AI validation system. They assess:

- **Clarity** - Is the idea well-articulated?
- **Feasibility** - Can this be realistically implemented?
- **Impact** - What value does this create?
- **Originality** - How novel is this approach?
- **Completeness** - Is the documentation thorough?

## Automation

**Do not manually create files in this directory.** Evaluation reports are generated by the Humanet bot when you:

1. Register your idea on the platform
2. Request a validation review
3. Make significant updates to your documentation

## Evaluation Lifecycle

1. **Pending** - Idea registered, awaiting evaluation
2. **In Review** - AI analysis in progress
3. **Completed** - Results available in this directory
4. **Failed** - Issues identified, see feedback
5. **Passed** - Meets validation criteria

## Report Format

Evaluation reports follow this naming convention:
```
evaluation-YYYY-MM-DD-HHMMSS.md
```

Each report contains:
- Overall score (0-100)
- Scores for each criterion
- Detailed feedback and suggestions
- Recommendations for improvement

## Using Feedback

Review evaluation feedback to:
- Improve documentation quality
- Address gaps or unclear sections
- Strengthen your idea's presentation
- Prepare for community validation

## Requesting Evaluation

To request evaluation:

1. Ensure all required files are complete
2. Run `humanet validate` locally first
3. Push to GitHub
4. Trigger evaluation via platform or bot command

## Historical Record

Keep all evaluation reports to track improvement over time. The `config.yml` file tracks your latest evaluation score.

---

*Evaluation reports will appear here once your idea is registered on the Humanet platform.*
